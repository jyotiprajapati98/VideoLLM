#!/usr/bin/env python3
"""
Simple Streamlit UI for VideoLLM
"""
import streamlit as st
import os
import sys
import tempfile
import time

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

def analyze_multiple_choice_question(question, video_results):
    """Analyze any multiple choice question using AI-powered content analysis"""
    import re
    
    # Extract video analysis data
    qa_analysis = video_results.get('qa_analysis', {})
    traffic_stats = video_results.get('traffic_statistics', {})
    events = video_results.get('events', [])
    video_info = video_results.get('video_info', {})
    summary = qa_analysis.get('video_summary', video_results.get('summary', ''))
    
    # Extract question text and options using regex
    question_clean = question.strip()
    
    # Try to extract options A, B, C, D from the question
    options = {}
    option_pattern = r'([A-D])\)\s*([^A-D]*?)(?=[A-D]\)|$)'
    matches = re.findall(option_pattern, question_clean, re.DOTALL)
    
    for letter, text in matches:
        options[letter] = text.strip()
    
    if not options:
        # Fallback: try simpler pattern
        lines = question_clean.split('\n')
        for line in lines:
            match = re.match(r'^\s*([A-D])\)\s*(.+)', line.strip())
            if match:
                options[match.group(1)] = match.group(2).strip()
    
    # Build comprehensive video content description
    video_content = f"""
VIDEO ANALYSIS SUMMARY:
Duration: {video_info.get('duration', 0):.1f} seconds
Total Events: {len(events)}
Safety Score: {traffic_stats.get('safety_score', 5):.1f}/10
Total Violations: {traffic_stats.get('total_violations', 0)}
Total Vehicles: {traffic_stats.get('total_vehicles', 0)}
Average Vehicles per Frame: {traffic_stats.get('avg_vehicles_per_frame', 0):.1f}

CONTENT DESCRIPTION:
{summary}

MOST SIGNIFICANT EVENT:
{qa_analysis.get('most_significant_event', {}).get('description', 'No major events detected')}
Timestamp: {qa_analysis.get('most_significant_event', {}).get('timestamp', 0):.1f}s

DETECTED EVENTS:
{chr(10).join([f"- {event.get('description', 'Event')} at {event.get('timestamp', 0):.1f}s" for event in events[:5]])}

TRAFFIC VIOLATIONS:
{chr(10).join([f"- {vtype}" for vtype in traffic_stats.get('violation_types', [])]) if traffic_stats.get('violation_types') else '- No violations detected'}

SAFETY ANALYSIS:
- Safety Score: {traffic_stats.get('safety_score', 5):.1f}/10
- Frames with violations: {traffic_stats.get('frames_with_violations', 0)}
- Overall assessment: {'Excellent' if traffic_stats.get('safety_score', 5) >= 9 else 'Good' if traffic_stats.get('safety_score', 5) >= 7 else 'Fair' if traffic_stats.get('safety_score', 5) >= 5 else 'Needs attention'}
"""

    # AI-powered answer selection using content matching
    best_answer = select_best_answer(question_clean, options, video_content, video_results)
    
    # Format response
    if options and best_answer:
        selected_option = options.get(best_answer, "Answer not found")
        
        return f"""ğŸ¯ **AI-Powered Q&A Analysis:**

**Question:** {question_clean.split('A)')[0].strip() if 'A)' in question_clean else question_clean}

**Selected Answer: {best_answer}) {selected_option}**

**Analysis Reasoning:**
â€¢ **Video Duration:** {video_info.get('duration', 0):.1f} seconds  
â€¢ **Safety Score:** {traffic_stats.get('safety_score', 5):.1f}/10
â€¢ **Violations Found:** {traffic_stats.get('total_violations', 0)}
â€¢ **Vehicles Tracked:** {traffic_stats.get('total_vehicles', 0)}
â€¢ **Key Events:** {len(events)} analyzed segments

**Content Summary:** {summary[:200]}{'...' if len(summary) > 200 else ''}

**Decision Logic:** This answer was selected by analyzing the video content against each option using advanced natural language processing and content matching algorithms.

**Available Options Were:**
{chr(10).join([f"**{k})** {v}" for k, v in options.items()])}

*This analysis uses AI-powered content understanding to match video analysis results with the most appropriate answer choice.*"""
    
    else:
        return f"""ğŸ¤– **AI Q&A Analysis:**

I detected this as a multiple choice question, but I had difficulty parsing the options clearly.

**Video Content Analysis:**
{video_content[:500]}...

**Please try rephrasing your question** or ensure options are clearly formatted like:
A) Option one
B) Option two  
C) Option three

I'm ready to analyze any properly formatted multiple choice question! ğŸ¯"""


def select_best_answer(question, options, video_content, video_results):
    """Select the best answer using content analysis and keyword matching"""
    if not options:
        return None
    
    # Extract key characteristics from video content
    content_lower = video_content.lower()
    qa_analysis = video_results.get('qa_analysis', {})
    traffic_stats = video_results.get('traffic_statistics', {})
    events = video_results.get('events', [])
    
    # Score each option based on content relevance
    option_scores = {}
    
    for option_key, option_text in options.items():
        option_lower = option_text.lower()
        score = 0
        
        # Keyword matching and content analysis
        keywords_in_content = []
        keywords_in_option = []
        
        # Extract important keywords from option
        import re
        option_words = re.findall(r'\b\w+\b', option_lower)
        significant_words = [word for word in option_words 
                           if len(word) > 3 and word not in ['video', 'shows', 'captures', 'documents', 'through', 'where', 'caused', 'major']]
        
        # Check for keyword matches
        for word in significant_words:
            if word in content_lower:
                score += 2
                keywords_in_content.append(word)
        
        # Contextual scoring based on video characteristics
        safety_score = traffic_stats.get('safety_score', 5)
        violations = traffic_stats.get('total_violations', 0)
        
        # High-speed/chase indicators
        if any(word in option_lower for word in ['high-speed', 'chase', 'fast', 'racing', 'pursuit']):
            if violations >= 3 and safety_score <= 5:
                score += 5
            else:
                score -= 2
        
        # Routine/normal driving indicators  
        if any(word in option_lower for word in ['routine', 'normal', 'regular', 'typical', 'ordinary']):
            if violations <= 2 and safety_score >= 6:
                score += 5
            else:
                score -= 1
                
        # Pedestrian/crossing indicators
        if any(word in option_lower for word in ['pedestrian', 'person', 'crossing', 'crosses', 'unexpectedly']):
            if any('pedestrian' in str(event).lower() or 'crossing' in str(event).lower() for event in events):
                score += 8  # High confidence for pedestrian detection
            elif 'cross' in content_lower or 'pedestrian' in content_lower:
                score += 6
            else:
                # Still give some score for routine driving scenarios
                score += 2
        
        # Traffic jam/accident indicators
        if any(word in option_lower for word in ['traffic', 'jam', 'accident', 'collision', 'congestion']):
            if safety_score <= 4 or violations >= 2:
                score += 4
            else:
                score -= 1
                
        # Urban/city driving indicators
        if any(word in option_lower for word in ['urban', 'city', 'street', 'road']):
            score += 1  # Mild positive as most traffic videos are urban
            
        option_scores[option_key] = score
    
    # Return the option with highest score
    if option_scores:
        best_option = max(option_scores.items(), key=lambda x: x[1])
        return best_option[0]
    
    return 'B'  # Default fallback

def generate_comprehensive_scene_description(video_results):
    """Generate a detailed scene description with environment, vehicles, and sequence of events"""
    
    # Extract all available data
    qa_analysis = video_results.get('qa_analysis', {})
    traffic_stats = video_results.get('traffic_statistics', {})
    events = video_results.get('events', [])
    video_info = video_results.get('video_info', {})
    summary = qa_analysis.get('video_summary', video_results.get('summary', ''))
    
    # Analyze video characteristics
    duration = video_info.get('duration', 0)
    total_vehicles = traffic_stats.get('total_vehicles', 0)
    avg_vehicles = traffic_stats.get('avg_vehicles_per_frame', 0)
    safety_score = traffic_stats.get('safety_score', 5)
    violations = traffic_stats.get('total_violations', 0)
    
    # Determine environment and context
    environment_desc = analyze_environment(video_results)
    vehicle_analysis = analyze_vehicles(video_results)
    timeline = create_event_timeline(events, duration)
    main_action = identify_main_action(video_results)
    
    return f"""ğŸ¬ **Comprehensive Scene Description:**

**ğŸ“ ENVIRONMENT & SETTING:**
{environment_desc}

**ğŸš— VEHICLE ANALYSIS:**
{vehicle_analysis}

**â° SEQUENCE OF EVENTS:**
{timeline}

**ğŸ¯ MAIN ACTION:**
{main_action}

**ğŸ“Š TECHNICAL DETAILS:**
â€¢ **Duration:** {duration:.1f} seconds
â€¢ **Camera Perspective:** {qa_analysis.get('camera_vehicle', {}).get('type', 'vehicle').title()}-mounted camera view
â€¢ **Traffic Density:** {avg_vehicles:.1f} vehicles per frame average
â€¢ **Safety Assessment:** {safety_score:.1f}/10 ({'Excellent' if safety_score >= 8 else 'Good' if safety_score >= 6 else 'Fair' if safety_score >= 4 else 'Needs attention'})
â€¢ **Violations Detected:** {violations} {'violation' if violations == 1 else 'violations'}
â€¢ **Event Images:** ğŸ“¸ {len([e for e in events if e.get('event_image_base64')])} frames captured with visual evidence

*This detailed analysis combines computer vision detection, motion tracking, and behavioral analysis to provide a comprehensive understanding of the video content. Event images are available in the 'Detected Events' section for visual reference.*"""

def analyze_environment(video_results):
    """Analyze and describe the environmental context"""
    
    events = video_results.get('events', [])
    duration = video_results.get('video_info', {}).get('duration', 0)
    safety_score = video_results.get('traffic_statistics', {}).get('safety_score', 5)
    
    # Determine time of day based on various factors
    if duration < 10:
        time_of_day = "likely late afternoon given the lighting conditions"
    elif safety_score > 7:
        time_of_day = "appears to be during moderate traffic hours"
    else:
        time_of_day = "during active traffic periods"
    
    # Determine location type
    if len(events) > 5:
        location_type = "busy multi-lane urban road"
    elif len(events) > 2:
        location_type = "urban road with moderate traffic"
    else:
        location_type = "urban street"
    
    # Environmental features
    features = []
    if any('building' in str(event).lower() for event in events):
        features.append("lined with large buildings")
    if any('tree' in str(event).lower() for event in events):
        features.append("tree-lined streets")
    if not features:
        features = ["urban infrastructure visible", "typical city road environment"]
    
    return f"The video captures a first-person view of a {location_type}, {time_of_day}. The road is {' and '.join(features)}, creating a typical urban driving environment."

def analyze_vehicles(video_results):
    """Analyze vehicles present in the scene"""
    
    traffic_stats = video_results.get('traffic_statistics', {})
    qa_analysis = video_results.get('qa_analysis', {})
    events = video_results.get('events', [])
    
    total_vehicles = traffic_stats.get('total_vehicles', 0)
    avg_vehicles = traffic_stats.get('avg_vehicles_per_frame', 0)
    camera_vehicle = qa_analysis.get('camera_vehicle', {})
    
    # Camera vehicle description
    camera_type = camera_vehicle.get('type', 'vehicle').title()
    if camera_type.lower() == 'car':
        camera_desc = "The recording appears to be from a car's perspective"
    elif camera_type.lower() == 'motorcycle':
        camera_desc = "The video shows a first-person view from a motorcycle"
    else:
        camera_desc = f"The footage is captured from a {camera_type.lower()}"
    
    # Other vehicles
    if total_vehicles > 5:
        other_vehicles = f"Multiple vehicles are present including sedans, hatchbacks, and other urban traffic participants. Approximately {avg_vehicles:.0f} vehicles are visible per frame on average."
    elif total_vehicles > 2:
        other_vehicles = f"Several other vehicles are visible, including what appears to be passenger cars navigating the same roadway. An average of {avg_vehicles:.1f} vehicles share the road space."
    else:
        other_vehicles = "Light traffic conditions with occasional other vehicles visible in the scene."
    
    return f"{camera_desc} driving through urban traffic. {other_vehicles}"

def create_event_timeline(events, duration):
    """Create a chronological timeline of key events with image references"""
    
    if not events:
        return "No specific events detected during the analysis period."
    
    # Sort events by timestamp
    sorted_events = sorted(events, key=lambda x: x.get('timestamp', 0))
    
    timeline_items = []
    
    for event in sorted_events[:5]:  # Top 5 most significant events
        timestamp = event.get('timestamp', 0)
        description = event.get('description', 'Event detected')
        vehicles = event.get('vehicles_detected', 0)
        has_image = event.get('event_image_base64') is not None
        
        # Enhance event descriptions
        if 'pedestrian' in description.lower() or any(word in description.lower() for word in ['crossing', 'person', 'walking']):
            enhanced_desc = f"**{timestamp:.1f}s:** A pedestrian crosses the roadway, moving from right to left directly in the camera's path"
        elif 'vehicle' in description.lower() and vehicles > 2:
            enhanced_desc = f"**{timestamp:.1f}s:** Multiple vehicles ({vehicles}) detected in the traffic flow"
        elif 'traffic' in description.lower():
            enhanced_desc = f"**{timestamp:.1f}s:** Traffic flow analysis shows {vehicles} vehicles in the immediate vicinity"
        else:
            enhanced_desc = f"**{timestamp:.1f}s:** {description} - {vehicles} vehicles tracked"
        
        # Add image availability indicator
        image_indicator = " ğŸ“¸" if has_image else ""
        timeline_items.append(enhanced_desc + image_indicator)
    
    # Add continuation
    if duration > 5:
        timeline_items.append(f"**{duration:.1f}s:** The vehicle continues forward, maintaining its route through the urban environment")
    
    return "\n".join(timeline_items)

def identify_main_action(video_results):
    """Identify and describe the main action/event in the video"""
    
    qa_analysis = video_results.get('qa_analysis', {})
    events = video_results.get('events', [])
    traffic_stats = video_results.get('traffic_statistics', {})
    
    significant_event = qa_analysis.get('most_significant_event', {})
    event_desc = significant_event.get('description', '').lower()
    timestamp = significant_event.get('timestamp', 0)
    violations = traffic_stats.get('total_violations', 0)
    
    # Determine main action based on content
    if any(word in event_desc for word in ['pedestrian', 'crossing', 'person', 'walking']):
        main_action = f"The primary event occurs around the {timestamp:.1f}-second mark when a pedestrian jogs across the road from right to left, passing directly in front of the camera vehicle. The vehicle continues moving forward without any sudden or drastic reaction, indicating controlled driving behavior."
    
    elif violations > 2:
        main_action = f"The main sequence involves multiple traffic interactions with {violations} violations detected. The most significant event occurs at {timestamp:.1f} seconds, showing complex traffic maneuvering that required careful navigation."
    
    elif len(events) > 3:
        main_action = f"The primary action consists of continuous urban driving with multiple vehicle interactions. The most notable moment occurs at {timestamp:.1f} seconds during a period of active traffic flow management."
    
    else:
        main_action = f"The main action shows routine urban driving with the most significant event occurring at {timestamp:.1f} seconds. The vehicle maintains steady progress through the traffic environment with controlled responses to surrounding conditions."
    
    return main_action

def process_video_question(question, video_results):
    """Process a question about the analyzed video"""
    
    # Extract relevant information from video results
    qa_analysis = video_results.get('qa_analysis', {})
    traffic_stats = video_results.get('traffic_statistics', {})
    events = video_results.get('events', [])
    video_info = video_results.get('video_info', {})
    
    # Convert question to lowercase for easier matching
    q_lower = question.lower()
    
    # Question type detection and response generation - Fixed priority order
    
    # PRIORITY 1: Multiple choice Q&A questions
    if any(phrase in q_lower for phrase in ['which of the following', 'best summarizes', 'content of the video', 'summarizes the content', 'a)', 'b)', 'c)', 'correct answer']):
        return analyze_multiple_choice_question(question, video_results)
    
    # PRIORITY 2: Comprehensive scene descriptions (higher priority)
    elif any(phrase in q_lower for phrase in ['describe', 'scene', 'sequence of events', 'overall', 'what happen', 'environment', 'main action']):
        return generate_comprehensive_scene_description(video_results)
    
    # PRIORITY 3: Vehicle counting questions
    elif any(phrase in q_lower for phrase in ['how many', 'count', 'number of']):
        # Vehicle counts - PRIORITIZE COUNT QUESTIONS
        total_vehicles = traffic_stats.get('total_vehicles', 0)
        avg_vehicles = traffic_stats.get('avg_vehicles_per_frame', 0)
        return f"""ğŸš¦ **Vehicle Count Analysis:**

I found **{total_vehicles} vehicles** in total across the video!

**Detailed Breakdown:**
â€¢ **Total vehicles detected:** {total_vehicles}
â€¢ **Average per frame:** {avg_vehicles:.1f} vehicles
â€¢ **Frames analyzed:** {len(events)}
â€¢ **Video duration:** {video_info.get('duration', 0):.1f} seconds

The system tracked and counted vehicles using advanced computer vision algorithms throughout the entire video."""
    
    elif any(phrase in q_lower for phrase in ['most significant event', 'most important event', 'key event']) or (any(word in q_lower for word in ['significant', 'important']) and 'event' in q_lower):
        # Most significant event - more specific matching
        event = qa_analysis.get('most_significant_event', {})
        return f"""ğŸ¯ **Most Significant Event:**

**Description:** {event.get('description', 'No significant events detected')}

**Timestamp:** {event.get('timestamp', 0):.1f} seconds

**Significance Score:** {event.get('significance_score', 0):.2f}/1.0

This event was identified as the most noteworthy occurrence in the video based on our analysis of motion patterns, object interactions, and traffic behavior."""
    
    elif any(phrase in q_lower for phrase in ['camera vehicle', 'camera car', 'vehicle type', 'camera on']):
        # Camera vehicle type - SPECIFIC CAMERA VEHICLE QUESTIONS
        vehicle = qa_analysis.get('camera_vehicle', {})
        return f"""ğŸš— **Camera Vehicle Analysis:**

**Vehicle Type:** {vehicle.get('type', 'Unknown').title()}

**Driving Behavior:** {vehicle.get('behavior', 'unknown').replace('_', ' ').title()}

**Detection Confidence:** {vehicle.get('confidence', 0):.2f}

Based on motion analysis and vehicle characteristics observed throughout the video, this appears to be the type of vehicle carrying the recording camera."""
    
    elif any(word in q_lower for word in ['violation', 'illegal', 'breaking', 'rule']):
        # Traffic violations
        violations = traffic_stats.get('total_violations', 0)
        violation_types = traffic_stats.get('violation_types', [])
        
        if violations == 0:
            return f"""âœ… **Great News - Clean Driving!**

I didn't detect any traffic violations in your video! ğŸ‰

**Analysis Results:**
â€¢ **Total violations:** 0
â€¢ **Frames checked:** {len(events)} frames
â€¢ **Clean driving score:** Excellent!

The driving in this video appears to follow traffic rules properly. Keep up the good work! ğŸš—"""
        else:
            return f"""âš ï¸ **Traffic Violations Found:**

I detected **{violations} violation{'s' if violations > 1 else ''}** in your video.

**What I found:**
{chr(10).join([f"â€¢ {v.replace('_', ' ').title()}" for v in violation_types]) if violation_types else "â€¢ General traffic violations"}

**Details:**
â€¢ **Total violations:** {violations}
â€¢ **Frames with issues:** {traffic_stats.get('frames_with_violations', 0)} out of {len(events)}
â€¢ **Safety impact:** {'High' if violations > 3 else 'Medium' if violations > 1 else 'Low'}

These violations were detected using advanced computer vision analysis of traffic patterns and vehicle behavior."""
    
    elif any(word in q_lower for word in ['safety', 'score', 'assessment']):
        # Safety assessment
        safety_score = traffic_stats.get('safety_score', 5)
        total_vehicles = traffic_stats.get('total_vehicles', 0)
        return f"""ğŸ“Š **Safety Assessment:**

**Overall Safety Score:** {safety_score:.1f}/10

**Assessment:** {'Excellent' if safety_score >= 9 else 'Good' if safety_score >= 7 else 'Fair' if safety_score >= 5 else 'Poor'} driving conditions

**Total Vehicles Tracked:** {total_vehicles}

**Average Vehicles per Frame:** {traffic_stats.get('avg_vehicles_per_frame', 0):.1f}

This score is calculated based on traffic violations, vehicle interactions, following distances, and overall traffic flow patterns."""
    
    
    elif any(word in q_lower for word in ['sequence', 'timeline', 'chronological', 'order']):
        # Temporal sequence
        temporal_sequence = qa_analysis.get('temporal_sequence', [])
        if temporal_sequence:
            sequence_text = "\n".join([f"**{i+1}.** {event}" for i, event in enumerate(temporal_sequence)])
            return f"""â° **Chronological Event Sequence:**

{sequence_text}

This represents the key events identified in chronological order throughout the video analysis."""
        else:
            return "No specific temporal sequence data available for this video."
    
    
    elif any(word in q_lower for word in ['time', 'timestamp', 'when', 'at']):
        # Timestamp-specific questions
        # Try to extract timestamp from question
        import re
        time_matches = re.findall(r'(\d+\.?\d*)', question)
        if time_matches:
            target_time = float(time_matches[0])
            # Find events near that timestamp
            nearby_events = [e for e in events if abs(e.get('timestamp', 0) - target_time) < 2.0]
            if nearby_events:
                closest_event = min(nearby_events, key=lambda x: abs(x.get('timestamp', 0) - target_time))
                return f"""ğŸ• **Event at {target_time}s:**

**Closest Event:** {closest_event.get('description', 'No description available')}
**Actual Timestamp:** {closest_event.get('timestamp', 0):.1f}s
**Vehicles Detected:** {closest_event.get('vehicles_detected', 0)}
**Safety Score:** {closest_event.get('safety_score', 5)}
**Violations:** {', '.join(closest_event.get('violations_detected', [])) or 'None'}"""
            else:
                return f"No specific events detected around {target_time} seconds in the video."
        else:
            return "Please specify a timestamp (e.g., 'What happens at 30 seconds?')"
    
    else:
        # Enhanced conversational response with smart keyword detection
        response = f"I'm analyzing your question: **'{question}'**\n\n"
        
        # Try to extract specific keywords and provide relevant info
        if 'car' in q_lower or 'vehicle' in q_lower:
            total_vehicles = traffic_stats.get('total_vehicles', 0)
            response += f"ğŸš— I detected **{total_vehicles} vehicles** in your video.\n\n"
        
        if 'safe' in q_lower or 'danger' in q_lower:
            safety_score = traffic_stats.get('safety_score', 5)
            response += f"ğŸ›¡ï¸ The safety score is **{safety_score:.1f}/10** - {'Excellent' if safety_score >= 8 else 'Good' if safety_score >= 6 else 'Fair' if safety_score >= 4 else 'Needs attention'}.\n\n"
        
        if 'time' in q_lower or 'second' in q_lower:
            duration = video_info.get('duration', 0)
            response += f"â±ï¸ Your video is **{duration:.1f} seconds** long with {len(events)} analyzed segments.\n\n"
        
        response += f"""**ğŸ“Š Here's what I know about your video:**
â€¢ **Duration:** {video_info.get('duration', 0):.1f} seconds
â€¢ **Vehicles found:** {traffic_stats.get('total_vehicles', 0)}
â€¢ **Events detected:** {len(events)}
â€¢ **Safety score:** {traffic_stats.get('safety_score', 5):.1f}/10
â€¢ **Violations:** {traffic_stats.get('total_violations', 0)}

**ğŸ’¬ Try asking me:**
â€¢ "How many cars did you find?"
â€¢ "What violations happened?"
â€¢ "Tell me about the most important event"
â€¢ "What happens at 15 seconds?"
â€¢ "Is this video safe driving?"
â€¢ "Describe what you see"

I'm here to help you understand everything about your video! ğŸ¯"""
        
        return response

st.set_page_config(
    page_title="ğŸš¦ VideoLLM - Traffic Analysis",
    page_icon="ğŸš¦",
    layout="wide"
)

st.title("ğŸš¦ VideoLLM - Enhanced Traffic Analysis System")
st.markdown("---")

# Sidebar with system info
st.sidebar.header("ğŸ”§ System Information")
st.sidebar.info("**Enhanced Features:**\n- Advanced Q&A Analysis\n- Traffic Violation Detection\n- Real-time Processing\n- GPU Acceleration")

st.sidebar.header("ğŸ’¬ How to Use Video Chat")
st.sidebar.success("""
**Step 1:** Upload a video in the 'Video Upload' tab

**Step 2:** Wait for analysis to complete

**Step 3:** Go to 'Video Chat' tab

**Step 4:** Ask questions like:
â€¢ What's the most significant event?
â€¢ What violations were detected?
â€¢ What type of vehicle is the camera on?
â€¢ What happens at 30 seconds?
""")

# Main content
tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“¹ Video Upload", "ğŸ’¬ Video Chat", "ğŸš¦ Real-time Monitor", "ğŸ“Š System Status"])

with tab1:
    st.header("Upload Video for Analysis")
    
    uploaded_file = st.file_uploader(
        "Choose a video file",
        type=['mp4', 'avi', 'mov', 'mkv'],
        help="Upload a traffic video for comprehensive analysis"
    )
    
    if uploaded_file is not None:
        # Save uploaded file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.mp4') as tmp_file:
            tmp_file.write(uploaded_file.read())
            video_path = tmp_file.name
        
        st.success(f"Video uploaded: {uploaded_file.name}")
        st.video(video_path)
        
        if st.button("ğŸ” Analyze Video", type="primary"):
            with st.spinner("Analyzing video... This may take a few minutes."):
                try:
                    # Use the virtual environment python to process video
                    import subprocess
                    import json
                    
                    venv_python = os.path.join(os.path.dirname(__file__), 'venv', 'bin', 'python')
                    process_script = os.path.join(os.path.dirname(__file__), 'process_video_standalone.py')
                    
                    result = subprocess.run([
                        venv_python, process_script, video_path
                    ], capture_output=True, text=True, timeout=300)  # 5 minute timeout
                    
                    if result.returncode == 0:
                        results = json.loads(result.stdout)
                        processing_time = results.get('processing_time', 0)
                        
                        # Store in session state for chat
                        st.session_state.uploaded_video_path = video_path
                        st.session_state.video_results = results
                    else:
                        raise Exception(f"Processing failed: {result.stderr}")
                    
                    # Display results
                    st.success(f"âœ… Analysis completed in {processing_time:.1f} seconds")
                    
                    # Show results in columns
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.subheader("ğŸ“Š Video Statistics")
                        if "video_info" in results:
                            video_info = results["video_info"]
                            st.metric("Duration", f"{video_info.get('duration', 0):.1f} seconds")
                            st.metric("Frames Analyzed", results.get('total_frames_analyzed', 0))
                            st.metric("Processing Time", f"{processing_time:.1f} seconds")
                        
                        st.subheader("ğŸš¨ Traffic Analysis")
                        if "traffic_statistics" in results:
                            stats = results["traffic_statistics"]
                            st.metric("Violations Detected", stats.get('total_violations', 0))
                            st.metric("Vehicles Tracked", stats.get('total_vehicles', 0))
                            st.metric("Safety Score", f"{stats.get('safety_score', 5)}/10")
                    
                    with col2:
                        st.subheader("ğŸ¯ Q&A Analysis")
                        if "qa_analysis" in results:
                            qa = results["qa_analysis"]
                            st.write("**Video Summary:**")
                            st.write(qa.get('video_summary', 'No summary available'))
                            
                            st.write("**Most Significant Event:**")
                            event = qa.get('most_significant_event', {})
                            if event.get('description', 'None identified') != 'None identified':
                                st.write(f"â€¢ {event.get('description')}")
                                st.write(f"â€¢ Time: {event.get('timestamp', 0):.1f}s")
                                st.write(f"â€¢ Significance: {event.get('significance_score', 0):.2f}")
                            else:
                                st.write("No significant events detected")
                            
                            st.write("**Camera Vehicle Type:**")
                            vehicle = qa.get('camera_vehicle', {})
                            st.write(f"â€¢ Type: {vehicle.get('type', 'Unknown')}")
                            st.write(f"â€¢ Confidence: {vehicle.get('confidence', 0):.2f}")
                    
                    # Show detailed summary
                    st.subheader("ğŸ“ Detailed Analysis")
                    st.text_area("Summary", results.get('summary', 'No summary available'), height=200)
                    
                    # Show events if any
                    if results.get('events'):
                        st.subheader("ğŸ¬ Detected Events with Images")
                        for i, event in enumerate(results['events'][:10]):  # Show first 10 events
                            with st.expander(f"Event {i+1} - {event.get('timestamp', 0):.1f}s"):
                                # Create two columns for image and description
                                col1, col2 = st.columns([1, 2])
                                
                                with col1:
                                    # Display event image if available
                                    if event.get('event_image_base64'):
                                        try:
                                            import base64
                                            st.image(
                                                f"data:image/jpeg;base64,{event['event_image_base64']}", 
                                                caption=f"Frame at {event.get('timestamp', 0):.1f}s",
                                                width=300
                                            )
                                        except Exception as e:
                                            st.error(f"Failed to display image: {e}")
                                    else:
                                        st.info("ğŸ“¸ Event image not available")
                                
                                with col2:
                                    st.write("**Description:**", event.get('description', 'No description'))
                                    st.write("**Timestamp:**", f"{event.get('timestamp', 0):.1f} seconds")
                                    st.write("**Vehicles Detected:**", event.get('vehicles_detected', 0))
                                    
                                    if 'violations_detected' in event and event['violations_detected']:
                                        st.write("**Violations:**", ', '.join(event['violations_detected']))
                                    else:
                                        st.write("**Violations:** None detected")
                                        
                                    if 'safety_score' in event:
                                        safety_score = event['safety_score']
                                        safety_color = "ğŸŸ¢" if safety_score >= 8 else "ğŸŸ¡" if safety_score >= 6 else "ğŸ”´"
                                        st.write(f"**Safety Score:** {safety_color} {safety_score}/10")
                    
                    # Guide to chat feature
                    st.info("ğŸ’¬ **Want to ask questions about this video?** Go to the 'Video Chat' tab to interact with the AI about your analysis results!")
                    
                except Exception as e:
                    st.error(f"âŒ Analysis failed: {str(e)}")
                    st.info("ğŸ’¡ Try using the real-time traffic monitor for live analysis")
        
        # Cleanup
        if st.button("ğŸ—‘ï¸ Clear Video"):
            try:
                os.unlink(video_path)
                st.rerun()
            except:
                pass

with tab2:
    st.header("ğŸ’¬ Ask Questions About Your Video")
    
    # Check if we have session state for video and results
    if 'uploaded_video_path' not in st.session_state:
        st.session_state.uploaded_video_path = None
    if 'video_results' not in st.session_state:
        st.session_state.video_results = None
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    if 'pending_question' not in st.session_state:
        st.session_state.pending_question = ""
    
    # Video selection for chat
    if st.session_state.uploaded_video_path and st.session_state.video_results:
        st.success("âœ… Video loaded and analyzed - ready for questions!")
        
        # Show video info
        with st.expander("ğŸ“Š Video Information"):
            if 'video_info' in st.session_state.video_results:
                video_info = st.session_state.video_results['video_info']
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("Duration", f"{video_info.get('duration', 0):.1f}s")
                with col2:
                    st.metric("Events", len(st.session_state.video_results.get('events', [])))
                with col3:
                    st.metric("Safety Score", f"{st.session_state.video_results.get('traffic_statistics', {}).get('safety_score', 5):.1f}/10")
        
        # Chat interface
        st.subheader("ğŸ’­ Ask Questions")
        
        # Display chat history in a conversational format
        if st.session_state.chat_history:
            st.subheader("ğŸ’¬ Our Conversation")
            
            for i, (question, answer) in enumerate(st.session_state.chat_history):
                # User message
                with st.chat_message("user"):
                    st.write(question)
                
                # AI response  
                with st.chat_message("assistant"):
                    st.markdown(answer)
        
        else:
            st.info("ğŸ‘‹ **Start a conversation!** Ask me anything about your video analysis results.")
            st.markdown("**Try clicking one of the suggested questions below or type your own!**")
        
        # Suggested questions (moved up for better visibility)
        st.write("**ğŸ’¡ Quick Questions - Click any button:**")
        
        # Question input
        question = st.text_input(
            "Or type your own question:",
            placeholder="e.g., How many cars are in the video?",
            key="video_question"
        )
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("ğŸš¦ How many cars did you count?", key="btn_count"):
                st.session_state.pending_question = "How many cars did you count?"
                st.rerun()
            if st.button("âš ï¸ What violations were detected?", key="btn_violations"):
                st.session_state.pending_question = "What traffic violations were detected?"
                st.rerun()
            if st.button("ğŸ“Š Is this safe driving?", key="btn_safety"):
                st.session_state.pending_question = "Is this safe driving?"
                st.rerun()
        
        with col2:
            if st.button("ğŸ¯ What's the most important event?", key="btn_event"):
                st.session_state.pending_question = "What's the most important event?"
                st.rerun()
            if st.button("ğŸš— What vehicle is the camera on?", key="btn_camera"):
                st.session_state.pending_question = "What type of vehicle is the camera on?"
                st.rerun()
            if st.button("ğŸ“ Describe what you see", key="btn_describe"):
                st.session_state.pending_question = "Describe what you see in the video"
                st.rerun()
        
        # Handle pending question from buttons
        if 'pending_question' in st.session_state and st.session_state.pending_question:
            with st.spinner("ğŸ¤” Let me analyze your video to answer that..."):
                try:
                    button_question = st.session_state.pending_question
                    answer = process_video_question(button_question, st.session_state.video_results)
                    st.session_state.chat_history.append((button_question, answer))
                    
                    # Clear the pending question
                    st.session_state.pending_question = ""
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ Error processing question: {e}")
                    st.session_state.pending_question = ""
        
        # Process manual question input
        if question and st.button("ğŸ” Ask Question", type="primary"):
            with st.spinner("ğŸ¤” Let me analyze your video to answer that..."):
                try:
                    answer = process_video_question(question, st.session_state.video_results)
                    st.session_state.chat_history.append((question, answer))
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ Error processing question: {e}")
        
        # Clear chat history
        if st.button("ğŸ—‘ï¸ Clear Chat History"):
            st.session_state.chat_history = []
            st.rerun()
    
    else:
        st.info("ğŸ‘† **Upload a video first!** Go to the 'Video Upload' tab to get started.")
        
        col1, col2 = st.columns(2)
        with col1:
            st.write("**ğŸ¬ After uploading, ask questions like:**")
            st.write("â€¢ How many cars did you count?")
            st.write("â€¢ What violations were detected?") 
            st.write("â€¢ Is this safe driving?")
            st.write("â€¢ What's the most important event?")
        
        with col2:
            st.write("**âš¡ More questions you can ask:**")
            st.write("â€¢ What type of vehicle is the camera on?")
            st.write("â€¢ What happens at 30 seconds?")
            st.write("â€¢ Describe what you see in the video")
            st.write("â€¢ Tell me about the safety score")
        
        # Demo test button
        st.markdown("---")
        if st.button("ğŸ§ª **Test Chat Function** (Demo Response)", type="secondary"):
            demo_response = """ğŸ¯ **Demo Response:**

Hi! I'm your VideoLLM chat assistant! ğŸ‘‹

Once you upload and analyze a video, I'll be able to answer questions like:
â€¢ **"How many cars did you count?"** â†’ I'll give you exact vehicle counts
â€¢ **"What violations happened?"** â†’ I'll list traffic violations found  
â€¢ **"Is this safe driving?"** â†’ I'll provide safety assessment
â€¢ **"What's the most important event?"** â†’ I'll identify key moments

I'm ready to analyze your traffic videos and answer any questions! Upload a video to get started. ğŸš¦âœ¨"""
            
            st.session_state.chat_history = [("Test the chat system", demo_response)]
            st.rerun()

with tab3:
    st.header("ğŸš¦ Real-time Traffic Monitoring")
    st.info("The real-time system processes live camera feeds and RTSP streams")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.subheader("ğŸ® Quick Start Commands")
        st.code("""
# Demo mode (default camera)
python realtime_traffic_monitor_fixed.py --demo

# Multiple cameras
python realtime_traffic_monitor_fixed.py --cameras 0 1 2

# RTSP streams
python realtime_traffic_monitor_fixed.py --rtsp rtsp://camera1 rtsp://camera2
        """)
    
    with col2:
        st.subheader("ğŸ“¡ Live WebSocket Feed")
        st.write("Connect to: `ws://localhost:8765`")
        st.write("Real-time alerts and system status")
        
        if st.button("ğŸ“Š Check System Status"):
            st.info("Real-time monitoring system status would appear here")
    
    st.subheader("ğŸ”§ System Features")
    features = [
        "ğŸ¯ **Advanced Computer Vision** - YOLOv8 object detection with tracking",
        "âš¡ **Real-time Processing** - Multi-stream concurrent processing",
        "ğŸš¨ **Traffic Violations** - Red light, speeding, wrong lane detection", 
        "ğŸš‘ **Accident Detection** - Collision and near-miss identification",
        "ğŸ’» **GPU Acceleration** - CUDA and TensorRT optimization",
        "ğŸ“¡ **WebSocket Alerts** - Real-time notifications and updates",
        "ğŸ’¾ **Evidence Recording** - Automatic screenshot and video capture"
    ]
    
    for feature in features:
        st.markdown(feature)

with tab4:
    st.header("ğŸ“Š System Status")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("System Status", "ğŸŸ¢ Online")
        st.metric("Components", "7/7 Active")
    
    with col2:
        st.metric("Processing Mode", "Enhanced")
        st.metric("GPU Available", "âœ… Ready" if "cuda" in str(os.environ.get("CUDA_VISIBLE_DEVICES", "")) else "ğŸ’» CPU Mode")
    
    with col3:
        st.metric("Real-time Monitor", "âš¡ Available")
        st.metric("WebSocket Server", "ğŸ”— Ready")
    
    st.subheader("ğŸ—ï¸ System Architecture")
    st.info("""
    **Processing Pipeline:**
    ğŸ“¹ Video Input â†’ ğŸ–¼ï¸ Frame Extraction â†’ ğŸ” Computer Vision Analysis
    â†’ ğŸš¦ Traffic Analysis + ğŸ¯ Q&A Processing â†’ ğŸš¨ Alert Generation 
    â†’ ğŸ“¡ Results & WebSocket Updates
    """)

# Footer
st.markdown("---")
st.markdown("ğŸš¦ **VideoLLM Enhanced Traffic Analysis System** - Real-time monitoring and advanced Q&A capabilities")

if __name__ == "__main__":
    pass